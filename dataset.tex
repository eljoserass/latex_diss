 
\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

%opening
\title{}
\author{Jose Rodriguez}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Dataset}
We used the same dataset for all the experiments accross this work. The GSM8K is an industry standard metric to evaluate LLMs promblem solving ability, which would allow us to do a fair comparison against the current state-of-the-art methods. To do this, it frames math world problems as a question answering task, which has been a long standing area of research in the NLP community. 

\paragraph{Question and Answering Generation (QA)}
In the context of this work we refer to QA as a task, where an LLM is prompted with question, and is expected to generate an answer. For the model to be trained with this task, it receives  a set of QA pairs, and the model parameters will be updated to optimize the loss between the generated answer and the expected answer. The trained model will then be evaluated on this task by introducint a sequence with the question and expecting the model to generate an answer, which validity will be verified differently depending on the task, in our case we discritly evaluate, it would be either correct or wrong and we use that information to compute the models accuracy.
\paragraph{Math World problems (MWP)}
A math word problem is a mathematical exercise where a significant part of the background information is presented in texts as natural language, rather than in mathematical notation (Part I ``Solving Arithmetic Mathematical Word Problems: A Review and Recent Advancements'' Chandra et al. (2019)). These problems are part of the basic elementary school curriculum, starting with basic operations(addition, subtraction, multiplication, division), and as the students advance problems contain higher levels of complexity (e.g., rate, probability, permutation, combination). Solving this problem requires linguistic and reasoning comprehension, thus the study of how children solve them has been a challenging area of research in cognitive science and education psychology (Part I ``Solving Arithmetic Mathematical Word Problems: A Review and Recent Advancements'' Chandra et al. (2019))). As solving them requires linguistic abilities and reasoning along with knowledge of arithmetic operations, researchers on AI and NLP have studied this as a task of building a system capable of replicating the cognitive process of solving these problems. 
\paragraph{Grade School Math 8k (GSM8K)}
Introduced by [cite], a dataset on the task of QA with 7.47k on the train and 1.32k on the test split. Each question is a MWP and the answer contain two pieces of information, an integer which is the target value and value a \textbf{gold} answer written by a humman, it contains the steps that lead to the integer, with relevant algebraic operations and their result. The steps in the gold answer are similary structured, and separeted by new lines, and at the end clearly state the precise integer which steps led to, this is what will be used as a label and will be used to determine if the answer is correct or not, independently of the steps. A metric on how ``hard'' a question is would be the amount of steps in the gold lead to the integer value. This dataset was a suitable candidate to test our hypothesis thanks to writing style that elicit in the model, which we conjecture that is esier to iterate as it can be approached atomically by the model by providing feedback to each step that led to the answer. Each problem is relatively unique, preventing the model from exploiting common patterns in the solving of the problems therefore allowing us to conduct better research on how the model can generalize its reasoning to solve the problems.



\subsection{Evaluation}
here just about how the evaluation is done

\end{document}
