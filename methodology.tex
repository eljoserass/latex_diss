\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

%opening
\title{}
\author{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Methodology}

\subsection{Train Set Filtering}
We identify the set of hardest questions by evaluating the learner model on the train set, which in this case is on the task of question answering of the GSM8K. For $N$ responses we extract the last number of the sequence which is matched again the expected numerical response. We consider this set as the hardest questions for the model in the dataset, which would be expressed as follow. $ hard_q =  N - correct_q $
In our case using LLAMA3-8B-IT with zero-shot CoT resulted on $ 2200 = 7400 - 51800 $ We proceed to iterate over this set, as the feedback to be provided assumes that the answer is incorrect, also as we conjecture that the parameters to the models are suficiently aligned to answer the easier questions.
\subsection{Data Generation}
The incorrect responses are repaired by a process of iteritevely exchange generation between two models, one for feedback and another one for refinement, which in practice is the same model with different prompting. This procedure is repeated over all the unsolved questions until a stop criteria is met. Somewehre here say that you add the attempts inspired by ILF.
\paragraph{Feedback Generation}
Each question has a set of feedback generated, which will be equal to the size of amount of iterations. Generation is done in chat format, firstly the model is instructed that the user will ask for feedback, and the answer is always wrong and that feedback must  be provided atomically identifying the incorrect steps in the answer. The role of the user contains the question that was attempted to answer, and the answer initially generated by the model, then feedback is generated as an assistant message. This is repeated through all the incorrect responses, iteritevely, replacing the initially wrong answer by the last refinement where the provided feedback was applied.
\paragraph{Refinement Generation}
Similar to feedback generation, each question will have a list of refinements which size will be equal of the amount of iterations. The model is intruscted that given a question from the user, and a incorrect answer from itself, the user will provide feedback and the model must apply the feedback and generate an improved version. Generation is done through a chat template and the messages from the user is the feedback generated in the previous step. The generation starts with the initially incorrect answer generated in the filtering step from the training set and the feedback generated for that answer by the feedback model, the process is then repeated iteritevely, adding the new refinements  attempts and the feedback for it to the chat until a correct solution is found.
\subsubsection{Stop Criteria}
The process of generating feedback and refinement for reach question will eaither stop because all question have a refinement with the correct answer, which is correctness is provided by the verifier, or because a max number of passes previously specified has been reached. Another edge case in which the process will end, which in our case was the main reason, is that the resources (like memory gpu) have been exhausted.
\paragraph{Verifier}
The verifier changes depending on the nature of the task, the main purpose is simply to provide the correctness of an answer, which in our case of QA, was simply a boolean if the answer was correct or not. This was computed by extracting the last numerical expresion of the string, and compare it to the expected number for that answer. This comparison can be strictly or flexibly made, the former must match a prefix and the exact number, the latter just needs to match the number, and this will match even if it has some symbol preceeding (eg. a dollar sign). We evaluate our mechanism mainly sing the latter. The same verifier is for the stop criteria and for the performance evaluation of the model, which comes from lm-eval-harness

    
\subsection{Training}
ss

\end{document}
