\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

%opening
\title{}
\author{Jose Rodriguez}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Results}
\subsection{Experiments}
big sft table with something like name / accuracy / instances / max sequence
name will probably have the sanitized or not / the completions only or not / the checkpoint / the name of the set


\paragraph{SFT Results}
Unless stated otherwise, all of the following experiments for 500 steps

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Name} & \textbf{Accuracy} & \textbf{Instances} \\
\hline
baseline & 0.6839 & Nan\\
baseline 8-shot CoT & 0.768 & Nan \\
hard-q solved sanitized& 0.6861 & 945\\
hard-q solved sanitized checkpoint-30 & 0.6884 & 945\\
hard-q solved sanitized checkpoint-140 & 0.6884 & 945\\
hard-q solved sanitized 50k steps &  0.548  & 946 \\
all solved (sanitized refinements only)& 0.7074 & 6333\\
all solved & 0.7028 & 6340\\
all solved sanitized & \textbf{0.7642}  & 6333\\
all solved sanitized + gold & 0.7391 & 7473 \\
all solved sanitized 8-shot CoT & 0.7915 & 6333 \\ 
hard-q solved sanitized + gold & 0.7005  & 2085\\
all solved (sanitized refinements only) + gold & 0.6945 & 7473 \\
hard-q gold & 0.7513 & 2085 \\
full gold & 0.7498 & 7473 \\
hard-q unsolved gold & 0.7445 & 1140 \\
hard-q unsolved gold checkpoint-30 & 0.6876 & 1140 \\
hard-q solved + gold with prefix & 0.6839 & 2085 \\
hard-q gold with prefix & 0.7225 & 2085 \\
easy-q & 0.6975 & 5388 \\ 
easy-q gold & 0.73 & 5388 \\
\hline
\end{tabular}
\caption{main table}
\end{table}

\paragraph{DPO Results}

\begin{table}[ht]
\centering
 \begin{tabular}{|l|c|c|}
 \hline
 \textbf{Name} & \textbf{Accuracy} & \textbf{Instances} \\
 \hline
 hard-q solved & 0.216 & 952 \\
 hard-q solved SFT + DPO & 0.2729 & 952 \\
 hard-q solved RPO [cite rpo] & 0.08491 & 952 \\
 \hline
 \end{tabular}
 \caption{small dpo table}
\end{table}


\section{Discussion}
In this section we mainly discuss two questions that caught our attention during the analysis of our experiments. Firstly, what is the lection to learn from on what makes some models better than others. Secondly, about the nature of the dataset, and how it is modelled, Is there a clear pattern what separate the kind of questions the model can and can not answer?
\subsection{What makes some models better than others?}
\subsubsection{Through the lens of of the cost function}
An interesting observation is how clear are the factors that affect the loss through the different experimental setups, where the difference between loss in experiments can be seen as a degree of familiarity with the data, were the generated answers for the easier questions have the smaller reported loss and the gold labels which are not generated by the model will have a higher loss, and in between the refinements that are generated by the model but not in the first pass but after multiple iterations. With this we identified that in most cases, smaller loss decrese accuracy and higher losses improve accuracy, which is a signal that the is farthest from over-fitting and generalizes better when the loss is higher.

\textbf{here image of the losses, easy-q, refinements, fullgold}

To further sustain this claim, with \textbf{hard-q solved sanitized 50k steps} we have tested running a model for 50k steps, which is x100 more iterations than all of the experimental setups. The idea of this experiment is to test the limits of overfitting, this model achieved a loss which was around 7 times smaller than the smallest loss on 500 steps, and 24 times smaller than the same set up over 500 steps. This model achieved an accuracy of 0.548, which is around 0.14 less than the same setup on 500 steps.

\textbf{NOTE plot grokking}


Another observation what makes the difference in between the top loss and the smaller loss. We have identified that mixing gold labels with synthetic data has a direct impact on the loss, as well as combining common pieces of text generated by the model with the gold labels and applying the same principle but in reverse, applying common pieces of text from the gold labels to the synthetic data. This changes are directly reflected in the loss function, and makes the value of the loss somwhere in between the smaller loss and the higher. 


\textbf{prefixes, sufixes, sanitazion and gold mix}


This insights provided by the loss were key to get to the highers performing model with synthetic data only, were the sanitization made the data more homogenous so the task between instances is more predictable, this helps the model generalize better as the model is optimzing to predict a more common pattern accross the taks. This homogenity throught instances may improve performance as the model is recieveing more loss signal from another sections of the sequence that are more responsible to produce better result. This sanitazion process also helped balance the loss to a point were was optimal for the model to produce better results, as the prefix added in the sanitation decressed the loss and the sufix increased it.

\textbf{NOTE filthy and sanitation plot}


Not only the expected generated text has a direct impact on the loss, but also the prompt used. When the model is prompted with easier questions, even if it is being trained on gold labels, the overall loss is smaller compared when is being trained on a set of harder questions. This can be observed directly by comparing the loss between the model trained on gold labels on the questions from easy-q and hard-q. Another way to observe this is by comparing the loss on the model when is being trained on the hard-q set and the loss on the unsolved hard-q set, this indicate that even though the model was not sufficiently aligned to answer the questions in the first zero-shot pass, it is more aligned to answer the questions that was able to refine after 7 passes (hard-q solved) than the ones that was not able to refine (hard-q unsolved)

\textbf{easy-q gold, hard-q gold, hard-q unsolved LOSS GRAPH}


Although in most cases higher loss means better accuracy, our observations has proven that this is not necesarly the case. For instance \textbf{hard-q solved sanitized checkpoint-30} is evaluated in a point when the accuracy is higher than the counterpart on 500 steps, and in this case, it leads to better accuracy, although by small margin. When this same hypothesis is tested with \textbf{hard-q solved unsolved gold checkpoint-30} the accuracy decreases by a margin of ~0.05. Another example of this phenomenom is in the difference in loss between \textbf{hard-q solved sanitized} and \textbf{all solved sanitized} where the latter has a significantly higehr accuracy by a margin of ~0.07 with a smaller loss. Motivated by the observation that models trained on gold data usually have higher accuracy with higher loss, we tried to apply this to \textbf{all solved sanitized + gold} were adding the gold labels would increase the overall loss and with it, the accuracy. We reported that this actually decreases the accuracy by ~0.02, having greatier impact in the accuracy decrease than a comparable experiment done with \textbf{all solved sanitized refinements only + gold}, that decreases the accuracy by less than ~0.01. 

\textbf{all the losses vertically}


Our intuitions from this results would be that when there is proportionally less examples of gold labels than generated ones, the model struggles to optimize a common pattern, as the gold labels are very different in structure when compared with generated labels. This is supported by two observations, firstly that when comparing \textbf{hard-q solved sanitized} and \textbf{hard-q solved sanitized + gold} in this case increases because there is proportionally more gold labels than generated ones, and this exposure allows the model to more accurately learn a pattern that usually leads to higher accuracy, likely thanks to the homogenous structure of the text. Secondly, this benefit from homogenous structure is seen on the sanitazion, where it improved all of the results where this was applied, except when this was applied on gold labels.

\textbf{here some pic comparing text of gold, sanitized and double sanitized}


\subsubsection{Through the lens of the accuracy}
If we want to understand how the model answer different questions, a good way is to ask if the accuracy is cummulative, meaning that if better models answer correctly all the answer correctly that a worse model would, in adition to the answers that the worse model could not get correctly. With this question we would be able to identify if the main driving force of improvement comes from a specific set of questions that a worse model was definetly not able to answer, and an improved version woul be able to. By comparing accuracy distribution accross the entire test set, between the \textbf{all solved sanitized} and \textbf{hard-q solved sanitized} and also between \textbf{all solved sanitized} and \textbf{baseline}, we have found that the accuracy is not entirely commulative, as the instances were the less capable model is capable to answer when the smarter model is not is non-zero. 
\begin{table}[ht]
\centering
 \begin{tabular}{|l|c|c|c|c|}
 \hline
 \textbf{Name} & \textbf{Accuracy} & \textbf{Draw} & \textbf{Win} & \textbf{Lose}\\
 \hline
 baseline & 0.6839 & 1085 & 64 & 170 \\
 hard-q solved sanitized & 0.6861 & 1068 & 74 & 177\\
 \hline
 \end{tabular}
 \caption{Difference against \textbf{all solved sanitized}, where \textit{Draw} represents the amount of times both models were equally correct or incorrect for a particular question, \textit{Win} represents the times when the less capable was correct on questions that the model with higher accuracy was not and \textit{Lose} represents the times when the more capable model was answer correctly and the less capable model did not}
\end{table}

This insight could mean that even though the distribution shift made during training, is mostly succesful as it better aligns the model toward corrects answers, at the same time this is also shifting away the model from some distributions that lead to correctness. In a way, this process is teaching the model how to correctly answer some answers that were previously incorrect, with the drawback that in some cases the model will forget how to correctly answer questions that previously was able to. 
\textbf{graph 1 and graph 2 here for the differences}

Another insight worth discussing is in the difference between draws, wins and loses of both models against the superior models. We can see that the baseline model has more draws, and less loses. Why is that? An answer to this question could be that, because the superior model is trained on the answers to easy questions, which would be similar, at least in style, to the answers that baseline model provided to the test set, the basline and the superior in some aspect are more similar. This also can be argumented when being in contrast with the \textbf{hard-q solved sanitized} model, which is being trained only on the answers that the model was not able to solve initially and was only able to answer after the refinement process, in a way this made the model more different from the baseline in comparison to the superior model.

This notion can be complemented if we compare the losses between \textbf{easy-q} and \textbf{hard-q solved sanitized}, where the loss for easy-q is significantly smaller than the loss of hard-q solved. The superior model was trained with a combination of easy-q and hard-q solved, with easy-q being around 5 times the size of hard-q solved. This supports the notion that the superior model was trained to predict more familiar data than it was to predict less familiar data, the latter being the refinements from hard-q solved.

To get a more clear view on what makes the models different is to keep looking to the difference between their answers.  Is there really a difference between the answers aside from the fact that the last number in the sequence waas correct or incorrect? There is two ways in which we could answer this question, by comparing the lenght and cosine similarity between the answers. When comparing the difference in sequence lenght difference between \textbf{all solved sanitized} and \textbf{hard-q solved sanitized}.

\begin{table}[ht]
\centering
 \begin{tabular}{|l|c|c|c|}
 \hline
 \textbf{Name} & \textbf{Median Diff Lenght} & \textbf{Mean Diff Lenght} & \textbf{Mean Cosine Similarity} \\
 \hline
 baseline & 18.00 & 31.69  & 0.2497 \\
 hard-q solved sanitized & -11.00 & -50.69 & 0.2751 \\
 \hline
 \end{tabular}
 \caption{Difference between models with \textbf{all solved sanitized}. }
\end{table}

Overall we see that the in this case the less capable model \textbf{hard-q solved sanitized} tends to produce longuer sequences than \textbf{all solved sanitized}, this goes in hand with the observation that, when comparing the \textbf{easy-q} dataset and \textbf{all solved sanitized} dataset, which combined form the dataset used for \textbf{all solved sanitized} we see that there is a clear difference in answer lenght, with a mean difference of -218.97 and median difference of -205.00. This difference in lenght can be attributed due to the conversational nature of the refinements, which are produced in a chat setting that iteritevly adds the previous answer as context in the conversation and is told to apply the feedback. 

When looking into the cosine similarity we see the cosine similarity of 0.249 for the basline and 0.2751 for hard-q solved sanitized, when computed between \textbf{all solved sanitized}, this indicate that responses between model are somewhat similar. Between easy-q and hard-q solved sanitized datasets, there is a cosine similarity of 0.27, this similarity could explain the cosine similarity observed between the responses of both finetuned versions at test time. With this observations we could conclude that the main driving factor of correctness for this case, is the lenght of the response, which it can be atributed to the nature of CoT of explaining the raitional to the final answer step-by-step, although as seen, a response too long could cause the model to produce a smaller accuracy. An explanation on why longer sequences could cause more errors may be that as the longer the sequence of steps, the more likely it is for the model to generate a token that could cause the reasoning chain to deviate from the correct answer.

\subsubsection {Through the lens of distributions}
An insight from the experiments is that it is ont always the case that more data leads to better results. When  using only gold data to train the model, we can see that \textbf{hard-q gold} has greater accuracy than \textbf{full gold}, using ~3 times less data. Another example of this can be seen on \textbf{hard-q unsolved gold} which is the smaller set that yields the best results, only with a small difference of ~0.006 in accuracy against \textbf{hard-q gold} being ~1.7 times smaller, and ~6.5 times smaller than \textbf{full gold}.

This is an example that shows the value of the filtering mechanism used to identify the hardest questions for the model. This is not only observed when trained with gold labels, as it also happens when using synthetic data only. The model \textbf{all solved sanitized} exhibits the superior performance when compared with models that use more data to train, such as \textbf{all solved sanitized + gold} or \textbf{all gold}.

A reason why the use of synthetic data might be better for this task than using gold labels, could be that as the model is already aligned to predict the data is trained on, the signal from the loss function could be focusing on being used to parameters that have a more a direct impact on computing the correct result in a math world problem, rather than, for instance focusing on optimizing writing style to be adapted as the writing in the gold labels. Another good reason to use synthetic data is that is easier to expand and improve, as it is dynamic there are ways to explore on how to improve the quality of this data generated from the model in the initial pass and the refinement step, being more cheap than collecting data from human crowdworkes. 

\subsection{What makes some questions harder than others?}
A simple way is to measure the following factors from the GSM8K, and compute the correlation between the answer being corret based on those metrics.

\begin{itemize}
 \item \textbf{Question Lenght} String size of the question.
 \item \textbf{Number of numbers in question} Count the amount of numbers present in the Math Word Problem question.
 \item \textbf{Number of gold steps} Gold answers are written by humans with a clear structure for each step separeted by a newline, count the new lines and use that as number of steps.
 \item \textbf{Number of number in gold answer}Count the amount of numbers present in the gold answer.
 \item \textbf{Gold answer lenght} String size of the gold answer
 \item \textbf{Mean step lenght}: Count the lenght of each step, and divide it by the amount of steps.
 \item \textbf{Operations in gold answer} Count the times each operation (+-/*) is used in the gold answer.
 \item \textbf{Total number of operations} Total amount of operations used in the gold answer.
 \item \textbf{Expected Label size} The amount of digits in the expected number in the answer.
\end{itemize}

The follwing table was extracted from the data collected in the initial filtering step of the training set, where we look at the full training set with paired with the gold answers, and a label wether the model was able to answer or not. We use this set to understand the nature on why the model is able to answer some questions and others is not.
\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{4pt}  % Reduce column separation
\begin{tabular}{|l|c|c|p{4.5cm}|}
\hline
\textbf{Factor} & \textbf{Correlation Coefficient} & \textbf{P-value} & \textbf{Interpretation} \\
\hline
Number of numbers in gold answer & -0.2022 & 0.0000 & Most challenging \\
Number of numbers in question & -0.1956 & 0.0000 & Very challenging \\
Gold answer length & -0.1831 & 0.0000 & Challenging \\
Number of gold steps & -0.1377 & 0.0000 & Moderately challenging \\
Mean step length & -0.1308 & 0.0000 & Moderately challenging \\
Question length & -0.1228 & 0.0000 & Moderately challenging \\
Total number of operations & -0.1065 & 0.0000 & Moderately challenging \\
Multiplication (*) operations & -0.0998 & 0.0000 & Slightly challenging \\
Division (/) operations & -0.0872 & 0.0000 & Slightly challenging \\
Subtraction (-) operations & -0.0100 & 0.3890 & Negligible effect (not statistically significant) \\
Addition (+) operations & -0.0088 & 0.4490 & Negligible effect (not statistically significant) \\
Expected label size & -0.0014 & 0.9013 & No significant effect \\
\hline
\end{tabular}
\caption{Factors affecting question difficulty in GSM8K dataset, sorted from most challenging to least challenging}
\label{tab:gsm8k_correlations}
\end{table}

With the results on Table \ref{tab:gsm8k_correlations} we can see a clear pattern that defines what can be considered a hard question from the model, where more numbers contained in the MWP directly makes the question harder, to answer. This is also sustained by the expected complexity of the answer by the number of characters and amount of numbers it contains. Interestingly, it is not as correlated the lenght of the question but the amount of numbers it contains, which makes sense as the model is able to filter the extra text and focus of the numbers as it would be the key complexity driver. Another interesting insight is that, although not as significant, is that questions that require more complex operations such as multiplication and division have a negative impact on the answer being solved, in comparison with simpler operations like addition and substraction that have barely any effect. It is also worth noting that the size of the number expected to be answered by the number has no effect on the model answering the question correctly, so the model will generaly will have the same ability to solve operations that result in a bigger number as well as a smaller number.

\subsubsection{What kind of questions was the model able to refine?}
An open question is also what makes different the questions that the model was able to refine after 7 iterations and the question that the model was not able to, as the model in this step does not necesarly will struggle with the same things as the initial pass. This will give us insight as how the model processes the refinement and also what kind of weakness can be improved for the feedback generated being used to refine the answer. The refinement process success rate on 7 passes is 45.32, which on one side is a good indication that refinement process is actually improving previously wrong answers, and on the other side makes us question what was the case for the wrong answers that were not able to be refined, and if it is only a matter of iterations to refine all answers or there is a more profound reason that makes the model nor being able to refine answers even after more passes.

By looking at Table \ref{tab:gsm8k_refinement_correlations} we see that the strongest factor that impedes the model to refine an answer is the amount of addition operations in the expected gold solution. This is counter-intiutive as in the initial pass, the type of operations performed have barely any effect, and if any the hardest questions were the ones that were expected to do divisions and multiplication not addition. In \ref{tab:gsm8k_correlations} we see that addition and substraction are very close to having the same effect on the question bein able to be answered or not, but in the case for refinements, there is a clear difference in the effect of addition and substraction. This insight could mean that the model is not necesarly generalazing well the addition operation, which did not seem to be a problem originally. This could also mean that, when the model is providing the feedback is not being able to correctly identify that the problem comes from an addition in the reasoning, so when the model is using the feedback to refine it is getting not enough signal to improve that part of the calculation. It could also mean that simply the questions that were not being able to be refined included a particularly hard sequence of additions, but further experiments are needed to sustain these claims. The rest of the factors that are hindering the refinement process, are similar to what it made the model to answer incorrectly in the first place, which relate to the complexity of the question, measured by the lenght, the amount of number and amount of gold steps. Interestingly, the model was able to refine questions with expected larger number, although this correlation is barely significant.
\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{4pt}  % Reduce column separation
\begin{tabular}{|l|c|c|p{4.5cm}|}
\hline
\textbf{Factor} & \textbf{Refinement Correlation} & \textbf{P-value} & \textbf{Interpretation} \\
\hline
Addition (+) operations & -0.1092 & 0.0000 & Most hindering to refinement \\
Question length & -0.0956 & 0.0000 & Hindering \\
Number of gold steps & -0.0946 & 0.0000 & Hindering \\
Gold answer length & -0.0913 & 0.0000 & Hindering \\
Number of numbers in gold answer & -0.0854 & 0.0001 & Hindering \\
Total operations & -0.0801 & 0.0003 & Moderately hindering \\
Number of numbers in question & -0.0634 & 0.0038 & Slightly hindering \\
Mean step length & -0.0459 & 0.0362 & Slightly hindering \\
Subtraction (-) operations & -0.0416 & 0.0573 & Very slightly hindering (not statistically significant) \\
Multiplication (*) operations & -0.0104 & 0.6346 & Negligible effect (not statistically significant) \\
Division (/) operations & -0.0080 & 0.7139 & Negligible effect (not statistically significant) \\
Expected label size & 0.0433 & 0.0480 & Slightly helping (barely statistically significant) \\
\hline
\end{tabular}
\caption{Factors affecting refinement success for initially incorrect answers in GSM8K dataset, sorted from most hindering to most helping}
\label{tab:gsm8k_refinement_correlations}
\end{table}

\textbf{NOTE GRAPHIC OF THE REFINEMENTS}

\subsubsection{What kind of answers was the model able to refine}
Another way to understand what enabled the model to refine a an answer, is not necesarly by the perspective of a what made a question harder, but by looking directly at the text, comparing the intially incorrect answer and succesful refinements, as there might patterns that the model is exploiting to refine the input sequence of the incorrect answer.

We can measure this by the following metrics:
\begin{itemize}
 \item Computing the cosine similarity \textbf{NOTE MAYBE ADD THIS IN BACKGROUND OR EVALUTION}between the incorrect answer and the refinement to see how semantically different are the answers from one another.
 \item Comparing the lenght of the string between each answer.
 \item Comparing the number of operations between the answers
\end{itemize}

By the measure of the cosine similarity, the results indicate that the answers are semantically very similar, which is a sign that the model is not necesarly exploiting a pattern of changing the style of the answer but instead fixing the particular incorrect operations.

\textbf{ADD HISOTGRAM COSINE SEMILARITY}

Most of the incorrect answers that are refined remain with similar lenght, although there are some cases were the scatter indicates that the answer is either longer or much shorter. For the case where the answer is much shorter, it can be attributed by the answers that were directly mapped by a sentence without describing the steps, this answers can also be attributed to the cases where there is significant diffrence in the cosine similarity. This answers were removed in the sanitazion process. For the answers that are longer it can be the case that the number of operations necessary increased, thereby the lenght of the answer also increased.

\textbf{NOTE ADD LENGHT SCATTER}

The number of operations vary, indicating that the refinement process is able to fix answers in various complexities.The most common pattern is the case where the refinement contains more operations than the incorrect counterpart, suggesting that the model is correcly identifying the missing steps and operations to achieve the answer. There is a significant amount of refinements that stays with the same number operations, this can be the case where the model fixed specific errors in the operations inside the steps, rather than adding more steps. Another commong pattern, although not as common as the previously mentioned, is the cases where the refinement contains less operations than the incorrect answer, meaning that the model might have been able to fix the reasoning chain by removing unnecesary operations and steps that lead to the correct answer.

\textbf{NOTE add number of operations plot}
\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
Metric & Original Incorrect & Refined Correct \\
\hline
Mean Cosine Similarity & \multicolumn{2}{c}{0.85} \\
Mean Answer Length & 615 & 731 \\
Mean Number of Operations & 4.2 & 5.8 \\
\hline
\end{tabular}
\caption{Summary Statistics of Original vs. Refined Answers}
\label{tab:summary}
\end{table}

Overall, these results show that the model is correctly applying the feedback necesary to refine the answer, where each question might have a different flawed step. This is a good sign of the generalization ability of the model for applying feedback, as it is not relying on a specific patter to exploit to reach the correct answer but rather following the instructions of the feedback to solve the previously incorrect answer.

\section{Conclusion}

\subsection{Future Work}

\end{document}
