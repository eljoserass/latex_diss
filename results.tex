\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

%opening
\title{}
\author{Jose Rodriguez}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Results}
\subsection{Experiments}
big sft table with something like name / accuracy / instances / max sequence
name will probably have the sanitized or not / the completions only or not / the checkpoint / the name of the set


\paragraph{SFT Results}
Unless stated otherwise, all of the following experiments for 500 steps

\begin{table}[ht]
\centering
\begin{tabular}{lcc}

Name & Accuracy & Instances \\
baseline & 0.6839 & Nan\\
baseline 8-shot CoT & 0.768 & Nan \\
hard-q solved sanitized& 0.6861 & 945\\
hard-q solved sanitized checkpoint-30 & 0.6884 & 945\\
hard-q solved sanitized checkpoint-140 & 0.6884 & 945\\
hard-q solved sanitized 50k steps &  0.548  & 946 \\
all solved (sanitized refinements only)& 0.7074 & 6333\\
all solved & 0.7028 & 6340\\
all solved sanitized & \textbf{0.7642}  & 6333\\
all solved sanitized 8-shot CoT & 0.7915 & 6333 \\ 
hard-q solved sanitized + gold & 0.7005  & 2085\\
all solved (sanitized refinements only) + gold & 0.6945 & 7473 \\
hard-q gold & 0.7513 & 2085 \\
full gold & 0.7498 & 7473 \\
hard-q unsolved gold & 0.7445 & 1140 \\
hard-q unsolved gold checkpoint-30 & 0.6876 & 1140 \\
hard-q solved + gold with prefix & 0.6839 & 2085 \\
hard-q gold with prefix & 0.7225 & 2085 \\
easy-q & 0.6975 & 5388 \\ 
easy-q gold & 0.73 & 5388 \\
\end{tabular}
\caption{main table}
\end{table}

\paragraph{DPO Results}

\begin{table}[ht]
\centering
 \begin{tabular}{lcc}
 Name & Accuracy & Instances \\
 hard-q solved & 0.216 & 952 \\
 hard-q solved SFT + DPO & 0.2729 & 952 \\
 hard-q solved RPO [cite rpo] & 0.08491 & 952 \\
 \end{tabular}
 \caption{small dpo table}
\end{table}


\section{Discussion}
In this section we mainly discuss two questions that caught our attention during the analysis of our experiments. Firstly, what is the lection to learn from on what makes some models better than others. Secondly, about the nature of the dataset, and how it is modelled, Is there a clear pattern what separate the kind of questions the model can and can not answer?
\subsection{What makes some models better than others?}
\subsubsection{Through the lens of of the cost function}
An interesting observation is how clear are the factors that affect the loss through the different experimental setups, where the difference between loss in experiments can be seen as a degree of familiarity with the data, were the generated answers for the easier questions have the smaller reported loss and the gold labels which are not generated by the model will have a higher loss, and in between the refinements that are generated by the model but not in the first pass but after multiple iterations. With this we identified that in most cases, smaller loss decrese accuracy and higher losses improve accuracy, which is a signal that the is farthest from over-fitting and generalizes better when the loss is higher.

\textbf{here image of the losses, easy-q, refinements, fullgold}

To further sustain this claim, with \textbf{hard-q solved sanitized 50k steps} we have tested running a model for 50k steps, which is x100 more iterations than all of the experimental setups. The idea of this experiment is to test the limits of overfitting, this model achieved a loss which was around 7 times smaller than the smallest loss on 500 steps, and 24 times smaller than the same set up over 500 steps. This model achieved an accuracy of 0.548, which is around 0.14 less than the same setup on 500 steps.

\textbf{NOTE plot grokking}


Another observation what makes the difference in between the top loss and the smaller loss. We have identified that mixing gold labels with synthetic data has a direct impact on the loss, as well as combining common pieces of text generated by the model with the gold labels and applying the same principle but in reverse, applying common pieces of text from the gold labels to the synthetic data. This changes are directly reflected in the loss function, and makes the value of the loss somwhere in between the smaller loss and the higher. 


\textbf{prefixes, sufixes, sanitazion and gold mix}


This insights provided by the loss were key to get to the highers performing model with synthetic data only, were the sanitization made the data more homogenous so the task between instances is more predictable, this helps the model generalize better as the model is optimzing to predict a more common pattern accross the taks. This homogenity throught instances may improve performance as the model is recieveing more loss signal from another sections of the sequence that are more responsible to produce better result. This sanitazion process also helped balance the loss to a point were was optimal for the model to produce better results, as the prefix added in the sanitation decressed the loss and the sufix increased it.

\textbf{NOTE filthy and sanitation plot}


Not only the expected generated text has a direct impact on the loss, but also the prompt used. When the model is prompted with easier questions, even if it is being trained on gold labels, the overall loss is smaller compared when is being trained on a set of harder questions. This can be observed directly by comparing the loss between the model trained on gold labels on the questions from easy-q and hard-q. Another way to observe this is by comparing the loss on the model when is being trained on the hard-q set and the loss on the unsolved hard-q set, this indicate that even though the model was not sufficiently aligned to answer the questions in the first zero-shot pass, it is more aligned to answer the questions that was able to refine after 7 passes (hard-q solved) than the ones that was not able to refine (hard-q unsolved)

\textbf{easy-q gold, hard-q gold, hard-q unsolved LOSS GRAPH}

\textbf{all the losses vertically}

\subsubsection{Through the lens of the accuracy}
If we want to understand how the model answer different questions, a good way is to ask if the accuracy is cummulative, meaning that if better models answer correctly all the answer correctly that a worse model would, in adition to the answers that the worse model could not get correctly. With this question we would be able to identify if the main driving force of improvement comes from a specific set of questions that a worse model was definetly not able to answer, and an improved version woul be able to. By comparing accuracy distribution accross the entire test set, between the \textbf{all solved sanitized} and \textbf{hard-q solved sanitized} and also between \textbf{all solved sanitized} and \textbf{baseline}, we have found that the accuracy is not entirely commulative, as the instances were the less capable model is capable to answer when the smarter model is not is non-zero. 
\begin{table}[ht]
\centering
 \begin{tabular}{lcccc}
 Name & Accuracy & Draw & Win & Lose\\
 baseline & 0.6839 & 1085 & 64 & 170 \\
 hard-q solved sanitized & 0.6861 & 1068 & 74 & 177\\
 \end{tabular}
 \caption{Difference against \textbf{all solved sanitized}, where \textit{Draw} represents the amount of times both models were equally correct or incorrect for a particular question, \textit{Win} represents the times when the less capable was correct on questions that the model with higher accuracy was not and \textit{Lose} represents the times when the more capable model was answer correctly and the less capable model did not}
\end{table}

This insight could mean that even though the distribution shift made during training, is mostly succesful as it better aligns the model toward corrects answers, at the same time this is also shifting away the model from some distributions that lead to correctness. In a way, this process is teaching the model how to correctly answer some answers that were previously incorrect, with the drawback that in some cases the model will forget how to correctly answer questions that previously was able to. 
\textbf{graph 1 and graph 2 here for the differences}

Another insight worth discussing is in the difference between draws, wins and loses of both models against the superior models. We can see that the baseline model has more draws, and less loses. Why is that? An answer to this question could be that, because the superior model is trained on the answers to easy questions, which would be similar, at least in style, to the answers that baseline model provided to the test set, the basline and the superior in some aspect are more similar. This also can be argumented when being in contrast with the \textbf{hard-q solved sanitized} model, which is being trained only on the answers that the model was not able to solve initially and was only able to answer after the refinement process, in a way this made the model more different from the baseline in comparison to the superior model.

This notion can be complemented if we compare the losses between \textbf{easy-q} and \textbf{hard-q solved sanitized}, where the loss for easy-q is significantly smaller than the loss of hard-q solved. The superior model was trained with a combination of easy-q and hard-q solved, with easy-q being around 5 times the size of hard-q solved. This supports the notion that the superior model was trained to predict more familiar data than it was to predict less familiar data, the latter being the refinements from hard-q solved.

To get a more clear view on what makes the models different is to keep looking to the difference between their answers.  Is there really a difference between the answers aside from the fact that the last number in the sequence waas correct or incorrect? There is two ways in which we could answer this question, by comparing the lenght and cosine similarity between the answers. When comparing the difference in sequence lenght difference between \textbf{all solved sanitized} and \textbf{hard-q solved sanitized}.

\begin{table}[ht]
\centering
 \begin{tabular}{lccc}
 Name  & Median Diff Lenght & Mean Diff Lenght & Mean Cosine Similarity \\
 baseline & 18.00 & 31.69  & 0.2497 \\
 hard-q solved sanitized & -11.00 & -50.69 & 0.2751 \\
 \end{tabular}
 \caption{Difference between models with \textbf{all solved sanitized}. }
\end{table}

Overall we see that the in this case the less capable model \textbf{hard-q solved sanitized} tends to produce longuer sequences than \textbf{all solved sanitized}, this goes in hand with the observation that, when comparing the \textbf{easy-q} dataset and \textbf{all solved sanitized} dataset, which combined form the dataset used for \textbf{all solved sanitized} we see that there is a clear difference in answer lenght, with a mean difference of -218.97 and median difference of -205.00. This difference in lenght can be attributed due to the conversational nature of the refinements, which are produced in a chat setting that iteritevly adds the previous answer as context in the conversation and is told to apply the feedback. 

When looking into the cosine similarity we see the cosine similarity of 0.249 for the basline and 0.2751 for hard-q solved sanitized, when computed between \textbf{all solved sanitized}, this indicate that responses between model are somewhat similar. Between easy-q and hard-q solved sanitized datasets, there is a cosine similarity of 0.27, this similarity could explain the cosine similarity observed between the responses of both finetuned versions at test time. With this observations we could conclude that the main driving factor of correctness for this case, is the lenght of the response, which it can be atributed to the nature of CoT of explaining the raitional to the final answer step-by-step, although as seen, a response too long could cause the model to produce a smaller accuracy. An explanation on why longer sequences could cause more errors may be that as the longer the sequence of steps, the more likely it is for the model to generate a token that could cause the reasoning chain to deviate from the correct answer.

\subsubsection {Through the lens of distributions}
\subsection{What makes some questions harder than others?}
\subsubsection{Lenght of the question}
\subsubsection{Amount of numbers in question}
\subsubsection{Amount of steps in gold answer}
\subsubsection{Lenght of the steps in gold}
\subsubsection{Operations in gold answer}
\subsubsection{Number sizes in gold answer}
\subsubsection{What kind of question-answer was the model able to refine?}



\end{document}
